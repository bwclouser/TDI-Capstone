{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "66f5b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d6d5fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/07/13 15:44:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local[*]','temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e07170e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e2d2592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd=sc.binaryRecords('/media/benjamin/Data/Chicago_Transit/TNP/201*.dat',96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4b8be76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=np.dtype([('id',np.ulonglong),('start year',np.short),('start month',np.byte),('start day',np.byte),('start hour',np.byte),('start minute',np.byte),('end year',np.short),('end month',np.byte),('end day',np.byte),('end hour',np.byte),('end minute',np.byte),('trip seconds',np.uint32),('trip miles',np.single),('pickup census tract',np.ulonglong),('dropoff census tract',np.ulonglong),('pickup community area',np.byte),('dropoff community area',np.byte),('fare',np.single),('tip',np.single),('addcharge',np.single),('trip total',np.single),('st auth',np.byte),('pool',np.byte),('pickup lat',np.double),('pickup lon',np.double),('dropoff lat',np.double),('dropoff lon',np.double)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4f0dfbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "129312197"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2b56375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_array(x):\n",
    "    array=np.frombuffer(bytes(x),dtype=dt)\n",
    "    #array=array.newbyteorder().byteswap() # big Endian\n",
    "    return array.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4c39d80c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(np.array([1,2,3]),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5f131cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=rdd.map(read_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "f4f22661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(41462311,\n",
       "  2019,\n",
       "  8,\n",
       "  30,\n",
       "  14,\n",
       "  0,\n",
       "  2019,\n",
       "  8,\n",
       "  30,\n",
       "  14,\n",
       "  15,\n",
       "  429,\n",
       "  2.4000000953674316,\n",
       "  0,\n",
       "  0,\n",
       "  60,\n",
       "  61,\n",
       "  5.0,\n",
       "  0.0,\n",
       "  2.549999952316284,\n",
       "  7.550000190734863,\n",
       "  0,\n",
       "  1,\n",
       "  41.8361501547,\n",
       "  -87.6487879519,\n",
       "  41.8090182499,\n",
       "  -87.6591665992),\n",
       " (41462313,\n",
       "  2018,\n",
       "  11,\n",
       "  27,\n",
       "  15,\n",
       "  0,\n",
       "  2018,\n",
       "  11,\n",
       "  27,\n",
       "  15,\n",
       "  45,\n",
       "  3308,\n",
       "  16.5,\n",
       "  0,\n",
       "  17031980000,\n",
       "  -1,\n",
       "  76,\n",
       "  30.0,\n",
       "  0.0,\n",
       "  7.5,\n",
       "  37.5,\n",
       "  0,\n",
       "  1,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  41.9790708201,\n",
       "  -87.9030396611),\n",
       " (41462314,\n",
       "  2018,\n",
       "  12,\n",
       "  6,\n",
       "  20,\n",
       "  15,\n",
       "  2018,\n",
       "  12,\n",
       "  6,\n",
       "  20,\n",
       "  45,\n",
       "  2131,\n",
       "  9.199999809265137,\n",
       "  0,\n",
       "  0,\n",
       "  -1,\n",
       "  19,\n",
       "  12.5,\n",
       "  0.0,\n",
       "  2.5,\n",
       "  15.0,\n",
       "  1,\n",
       "  2,\n",
       "  -1.0,\n",
       "  -1.0,\n",
       "  41.9272609555,\n",
       "  -87.7655016086)]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1c2d39f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "zz=out.toDF(['id','start year','start month','start day','start hour','start minute','end year','end month','end day','end hour','end minute','trip seconds','trip miles','pickup census tract','dropoff census tract','pickup community area','dropoff community area','fare','tip','addcharge','trip total','st auth','pool','pickup lat','pickup lon','dropoff lat','dropoff lon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "c23ccbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_jday = F.udf(lambda row: pd.Timestamp(row[0],row[1],row[2],row[3]).to_julian_date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "08f95806",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_dt = F.udf(lambda row: datetime(row[0],row[1],row[2],row[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "46517069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, start year: bigint, start month: bigint, start day: bigint, start hour: bigint, start minute: bigint, end year: bigint, end month: bigint, end day: bigint, end hour: bigint, end minute: bigint, trip seconds: bigint, trip miles: double, pickup census tract: bigint, dropoff census tract: bigint, pickup community area: bigint, dropoff community area: bigint, fare: double, tip: double, addcharge: double, trip total: double, st auth: bigint, pool: bigint, pickup lat: double, pickup lon: double, dropoff lat: double, dropoff lon: double, loop to ohare: boolean]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz.withColumn('loop to ohare',(zz['dropoff community area']==74) & (zz['pickup community area']==32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "557ab291",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeater = F.udf(lambda x: x[0]*365.25*24+(x[1]-1)*30.5*24+(x[2]-1)*24+x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "0aacf721",
   "metadata": {},
   "outputs": [],
   "source": [
    "zzt=zz.withColumn('repeated',to_dt(F.struct([zz[x] for x in zz.columns])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "87a55eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/07/16 16:31:44 ERROR Executor: Exception in task 0.0 in stage 61.0 (TID 3812)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_9512/3604848884.py\", line 1, in <lambda>\n",
      "ValueError: year 41462311 is out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/07/16 16:31:44 WARN TaskSetManager: Lost task 0.0 in stage 61.0 (TID 3812) (benjamin-precision executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_9512/3604848884.py\", line 1, in <lambda>\n",
      "ValueError: year 41462311 is out of range\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "22/07/16 16:31:44 ERROR TaskSetManager: Task 0 in stage 61.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_9512/3604848884.py\", line 1, in <lambda>\nValueError: year 41462311 is out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9512/3166540989.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mzzt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/home/benjamin/anaconda3/lib/python3.9/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"/tmp/ipykernel_9512/3604848884.py\", line 1, in <lambda>\nValueError: year 41462311 is out of range\n"
     ]
    }
   ],
   "source": [
    "zzt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "28fb0540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bd=F.struct([zz[x] for x in zz.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "fd5d7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=repeater(bd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8820e2a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ab=zz.groupby(['start year','start month','start day','start hour','pickup community area']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "1233ec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab32=ab.where(ab['pickup community area']==32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "eb5685f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:=====================================================>(376 + 2) / 378]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+---------+----------+---------------------+-----+-----------+\n",
      "|start year|start month|start day|start hour|pickup community area|count|   datetime|\n",
      "+----------+-----------+---------+----------+---------------------+-----+-----------+\n",
      "|      2019|          8|       12|        16|                   32| 2183|1.7703958E7|\n",
      "|      2018|         11|       20|        21|                   32| 1325|1.7697585E7|\n",
      "|      2018|         12|        3|        20|                   32| 1621|1.7697908E7|\n",
      "|      2018|         11|       19|        16|                   32| 1772|1.7697556E7|\n",
      "|      2019|          7|       21|        13|                   32| 1724|1.7703439E7|\n",
      "|      2018|         12|       19|        11|                   32| 1118|1.7698283E7|\n",
      "|      2018|         12|        9|        23|                   32| 1593|1.7698055E7|\n",
      "|      2019|          7|       11|         0|                   32|  496|1.7703186E7|\n",
      "|      2019|          8|       31|         2|                   32|  379|  1.77044E7|\n",
      "|      2018|         11|       18|         4|                   32|  142| 1.769752E7|\n",
      "|      2018|         12|       21|         9|                   32|  871|1.7698329E7|\n",
      "|      2019|          8|       23|        20|                   32| 1988|1.7704226E7|\n",
      "|      2018|         11|       26|        13|                   32|  949|1.7697721E7|\n",
      "|      2019|          8|       12|         0|                   32|  304|1.7703942E7|\n",
      "|      2018|         12|       27|        19|                   32| 1457|1.7698483E7|\n",
      "|      2019|          5|        8|         8|                   32| 1350|1.7701658E7|\n",
      "|      2019|          7|       16|        13|                   32| 1477|1.7703319E7|\n",
      "|      2019|          7|       13|        13|                   32| 1596|1.7703247E7|\n",
      "|      2018|         11|        5|         6|                   32|  353| 1.769721E7|\n",
      "|      2018|         11|       19|         4|                   32|  133|1.7697544E7|\n",
      "+----------+-----------+---------+----------+---------------------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ab32.withColumn('datetime',repeater(F.struct([ab32[x] for x in ab32.columns]))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b03ca1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "yr=2008\n",
    "mo=12\n",
    "dy=11\n",
    "hr=16\n",
    "mn=3\n",
    "date=datetime(yr,mo,dy,hr,mn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7a275c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=pd.Timestamp(2008,12,11,16,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cc25c134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2454812.1694444446"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.to_julian_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45436160",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = sqlContext.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
    "\n",
    "count_empty_columns = udf(lambda row: len([x for x in row if x == None]), IntegerType())\n",
    "\n",
    "new_df = df.withColumn(\"null_count\", count_empty_columns(struct([df[x] for x in df.columns])))\n",
    "\n",
    "new_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
